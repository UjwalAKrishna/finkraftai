# FinkraftAI Environment Configuration
# Copy this file to .env and configure your settings

# ===== LLM Provider Configuration =====
# Primary LLM Provider (gemini, openai, local)
LLM_PROVIDER=gemini

# Enable fallback to other providers if primary fails
LLM_FALLBACK_ENABLED=false

# Gemini Configuration (Primary)
GEMINI_API_KEY=
GEMINI_MODEL=gemini-1.5-flash
GEMINI_TEMPERATURE=0.7
GEMINI_MAX_TOKENS=1000

# OpenAI Configuration (Optional - for fallback or primary)
# OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-3.5-turbo
OPENAI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=1000

# Local LLM Configuration (LM Studio, Ollama, etc.)
LOCAL_LLM_URL=http://172.17.144.1:1234
LOCAL_MODEL=deepseek-r1-distill-llama-8b
LOCAL_TEMPERATURE=0.7
LOCAL_MAX_TOKENS=1000

# ===== General Settings =====
DATABASE_URL=sqlite:///./database/finkraft.db
SECRET_KEY=your_secret_key_here
ENVIRONMENT=development

# ===== Example LLM Configurations =====
# 
# Configuration 1: Gemini Only
# LLM_PROVIDER=gemini
# GEMINI_API_KEY=your-key
# LLM_FALLBACK_ENABLED=false
#
# Configuration 2: OpenAI Primary with Gemini Fallback
# LLM_PROVIDER=openai
# OPENAI_API_KEY=your-openai-key
# GEMINI_API_KEY=your-gemini-key
# LLM_FALLBACK_ENABLED=false
#
# Configuration 3: Local LLM with Cloud Fallback
# LLM_PROVIDER=local
# LOCAL_LLM_URL=http://172.17.144.1:1234
# LOCAL_MODEL=your-local-model
# GEMINI_API_KEY=your-gemini-key
# LLM_FALLBACK_ENABLED=false